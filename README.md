# WRF installation 
This repository provides documentation on how to compile and install the WRF model, along with the necessary tools. It also outlines the workflow and explains how to set it up for automated, operational model runs. If you are testing WRF on your own computer, check how many CPUs are available. While the model can run on a single core, these instructions are intended for parallel computation. Also, ensure you have sufficient disk space - static geographical data requires approximately 100GB, and the model input/output/temporary files take up a similar amount, depending on your domain. For testing, 200-300GB should be sufficient, while around 1TB is recommended for operational runs. More comprehensive documentation about WRF model can be found [here](https://www2.mmm.ucar.edu/wrf/users/wrf_users_guide/build/html/index.html) 

## Automated Installation

To get started with automated installation:

1. Clone this repository:
   ```
   git clone git@github.com:fmidev/WRF_installation.git
   cd WRF_installation
   ```

2. Make the installation script executable:
   ```
   chmod +x installation.sh
   ```

3. Run the installation script. Specify installation directory inside the script (default: /home/$USER/WRF_Model):
   ```
   ./installation.sh 
   ```

4. The script will:
   - Install all required dependencies
   - Download and compile necessary libraries
   - Build WRF, WPS, WRFDA and UPP with appropriate configurations
   - Set up the environment variables
   - Create the directory structure for operational use and set up crontab template
   - Downloads static geographical data and CRTM coefficients for DA

## Manual Installation
The text file `installation` provides a step-by-step guide on how to install and compile all the needed libraries. Similar instructions are also provided for the WRF source code and its pre/post-processing tools. Following these instructions will ensure that all the necessary binaries for running the WRF model are installed correctly. When running the WRF model or its tools, it is necessary to define some environment variables. These are controlled by `env.sh` which is executed within the model run scripts. Note that in paths (both in installation and `env.sh`), `user` has to be replaced with the correct username. Be also sure that all needed programs were installed, like rsync, etc. (List available in beginning of installation instruction.)

## Domain maker
Desired domains for WRF can be easily drawn with [WRF Domain Wizard](https://wrfdomainwizard.net/).

1. Select option "New" from the sidebar and draw the domain on the map. Keep in mind that this will be the outermost domain, which will be calculated with a resolution based on dx/dy (default 12 km).
2. Once you're done, add an inner domain by clicking "Add Nest" and modify it on map.
3. Adjust the resolution of the inner domain by using the "Parent_grid_ratio" setting. The default value is 3, which corresponds to a resolution of 4 km (12 km / 3).
4. To download the finalized domain namelist (namelist.wps) for WRF, click "Save". The information is needed later when configuring WPS running scripts.

## Work flow

### Boundaries
WRF requires boundary files in GRIB format. The script `Download_GFS/get_gfs.sh` is used to download GFS data to the WRF server. The configuration file `Download_GFS/gfs.cnf` specifies the desired GFS area, resolution, valid hours, and other parameters for the download. The paths may need adjustment to work correctly for the specific case. The key is to place the GFS data in the directory specified in the running scripts (default `/home/{user}/WRF_Model/GFS`). Script is always downloading the most recent boundaries.

```
./get_gfs.sh
```

### Preprocessing
Before running WPS (WRF Preprocessing System), you must first acquire the necessary static geographical datasets (if installing manually). These datasets contain terrain elevations, land use categories, soil types, and other surface characteristics required by the model. 

```
# Download the complete geographical dataset (~100GB)
wget -P /path/to/WPS_GEOG/ https://www2.mmm.ucar.edu/wrf/src/wps_files/geog_complete.tar.gz 
# Download high-resolution mandatory data (~30GB)
wget -P /path/to/WPS_GEOG/ https://www2.mmm.ucar.edu/wrf/src/wps_files/geog_high_res_mandatory.tar.gz 
# Extract both archives, removing the top-level directory
cd /path/to/WPS_GEOG/
tar -zxvf geog_complete.tar.gz --strip-components=1
tar -zxvf geog_high_res_mandatory.tar.gz --strip-components=1
```

After acquiring the geographical data, you need to configure the WPS namelist parameters in `Run_scripts/Run_WPS.sh` and in `Run_scripts/Run_WRF.sh`. These settings define your model domains, resolution, and projection:

1. **Domain Configuration**: Transfer the parameters from your `namelist.wps` file (generated by WRF Domain Wizard) into the script. Pay special attention to:
   - Domain center coordinates (`ref_lat`, `ref_lon`)
   - Grid dimensions (`e_we`, `e_sn`)
   - Grid spacing (`dx`, `dy`)
   - Nesting ratios (`parent_grid_ratio`)
   - Map projection (`map_proj`)

2. **File Paths**: Customize all directory paths in the script to match your specific installation locations (should be already correct if using automated installation). This includes:
   - Path to WPS executables
   - Path to geographical data
   - Input/output directories

3. **Computational Resources**: Adjust the number of MPI processes based on your available computing resources. For example:
   ```
   mpirun -np 24 ./geogrid.exe  # Uses 24 CPU cores for parallel processing
   ```

You can test your WPS configuration by executing the script with appropriate parameters:
```
# Syntax: ./Run_WPS.sh <year> <month> <day> <hour> <leadtime> <prod_dir>
# Example: Run WPS for September 10, 2024, 01:00 UTC with 48-hour forecast
./Run_WPS.sh 2024 09 10 01 48 /home/username/WRF_Model/out/
```

Upon successful execution, the script will generate intermediate files and WRF input files required for the model simulation.

### Observations (Optional for Data Assimilation)
Data assimilation requires observational data to improve initial conditions. The system currently supports observations from the NCEP real-time database.

1. **Supported Data Types**: The included `get_obs.sh` script retrieves primarily satellite radiances and other global observations from [NCEP's real-time data repository](https://nomads.ncep.noaa.gov/pub/data/nccf/com/obsproc/prod/). 

2. **Data Limitations**: 
   - The NCEP repository typically only retains the most recent 2-3 days of observations
   - Local observations, radar data, and custom observation networks require additional preprocessing into Little_R format (not currently included)

3. **Retrieving Observations**: Execute the script with date/time parameters:
   ```
   # Syntax: ./get_obs.sh <year> <month> <day> <hour>
   # Example: Get observations for September 10, 2024, 01:00 UTC
   ./get_obs.sh 2024 09 10 01
   ```

4. **Output**: The script will download and organize observation files into the appropriate directory structure required by WRFDA.

### The Model
The WRF model simulation is managed through the `Run_WRF.sh` script, which orchestrates the entire workflow including optional data assimilation.

1. **Configuration**: Before running the model, thoroughly review and customize:
   - **Namelist Parameters**: Ensure the physics options, time steps, and domain specifications match your requirements and align with the WPS configuration. The complete reference for namelist options is available in the [WRF User's Guide](https://www2.mmm.ucar.edu/wrf/users/wrf_users_guide/build/html/namelist_variables.html).
   
   - **Directory Paths**: Verify all file paths are correctly set for your environment (should be already correct if using automated installation), including:
     - WRF executable location
     - Input/output directories
     - Working directories for each stage of processing
   
   - **Computational Resources**: Adjust the number of processors for optimal performance on your system
   
   - **Data Assimilation Settings**: If using WRFDA, review the DA-specific options in the script

2. **Execution**: Launch the model with appropriate parameters:
   ```
   # Syntax: ./Run_WRF.sh <year> <month> <day> <hour> <leadtime> <prod_dir>
   # Example: Run WRF for September 10, 2024, 01:00 UTC with 48-hour forecast
   ./Run_WRF.sh 2024 09 10 01 48 /home/username/WRF_Model/out/
   ```

3. **Data Assimilation**: To enable data assimilation:
   - Set `WRFDA=true` in your `env.sh` configuration file
   - Ensure CRTM coefficients are available for satellite data assimilation
   - Provide appropriate background error (BE) statistics

### Postprocessing
Unified Post Processor (UPP) can be used to convert WRF NetCDF output to Grib. Instructions how to compile UPP can be found from the `installation`. The text file `setup_upp` describes how to setup UPP as a WRF postprosessing tool (Not needed to setup separately if using automated installation) and with `Run_scripts/execute_upp.sh` the UPP can be easily used to automated NETCDF -> GRIB conversion.

### Verification
Instruction how to use HARP verification for WRF here... (some day)

### Cleaning and automization
The `clean_wrf` script cleans GFS boundary files, WRF, and UPP output files. This can be set up as a cron job, for example, to clean the files once a day.

For an automated run, the `control_run_WRF.sh` script can be used to execute all the steps mentioned above. However, it is recommended to test each part of the workflow independently before using this control script to run WRF. The script can be executed as follows:
```
./control_run_WRF.sh <analysis_hour>
```

